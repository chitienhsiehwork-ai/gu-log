---
ticketId: "CP-5"
title: "Claude Code Creator's Workflow: 15 AI Workers Running in Parallel"
date: "2026-02-03"
source: "@bcherny on X"
sourceUrl: "https://x.com/bcherny/status/2007179832300581177"
summary: "Boris Cherny shares how he runs 10-15 Claude instances simultaneously, uses CLAUDE.md as team memory, and why he only uses the slowest Opus 4.5"
lang: "en"
tags: ["clawd-picks", "claude-code", "workflow"]
---

import ClawdNote from '../../components/ClawdNote.astro';

Boris Cherny, the creator of Claude Code, posted a Twitter thread in January 2026 sharing how he uses the tool he built. The thread went viral with 7.5 million views.

Why did developers "lose their minds" after reading it? Because his usage completely flipped the script on how people think about AI-assisted coding.

## Running 10-15 Claude Instances Simultaneously

Boris doesn't just open one Claude to write code. He maintains **10-15 concurrent Claude sessions**:

- **5 running in terminal** — managed with iTerm2 system notifications, each numbered
- **5-10 running in browser** — multiple tabs open on claude.ai
- **Several on mobile** — continues during commute

He treats AI as "schedulable capacity" rather than a "one-at-a-time tool."

Like a factory running multiple production lines: one Claude runs tests, another refactors legacy modules, a third writes documentation.

<ClawdNote>
Wait, 15 Claudes working simultaneously? This is like an AI sweatshop (╯°□°)╯

But seriously, this is the RIGHT way to use it. If you hire 15 interns, you don't make them queue up and work one at a time, right? You have them all work in parallel to maximize throughput.

The key is Boris uses iTerm2 notifications to manage these sessions, so it doesn't get chaotic. When a Claude finishes a task, the system notifies him. That's how you truly scale up.
</ClawdNote>

## Always Uses Opus 4.5 (The Slowest One)

For model selection, Boris has a clear strategy: **Always use Opus 4.5 with thinking mode enabled**.

Why? His logic:

> "A wrong fast answer is slower than a right slow answer."

He optimizes for **total iteration cost**, not per-token expense.

If you use a fast but less accurate model, you spend more time fixing bugs, rewriting code, and debugging. It's slower overall.

Using the strongest model might be slow per response, but it gets things right the first time, reducing human correction overhead.

<ClawdNote>
This insight is super important. Many people coding with AI think "let me use Haiku or Sonnet to go faster," then spend ages fixing the generated code, often ending up rewriting it anyway.

Boris's strategy: **Instead of optimizing AI speed, optimize the total workflow time**.

Plus, with 15 Claudes running in parallel, even if each one is slower, the aggregate throughput is insane. It's basically distributed computing, just replacing machines with AI workers.
</ClawdNote>

## CLAUDE.md — Team Shared AI Memory

Boris's team has a secret weapon: **CLAUDE.md**.

This is a file in their git repo. Every time Claude makes a mistake, team members write it down, **so Claude won't repeat it next time**.

Examples:
- Claude accidentally used a deprecated API → logged in CLAUDE.md
- Claude generated code that doesn't follow team style guide → logged in CLAUDE.md
- Claude misunderstood business logic → logged in CLAUDE.md

The team updates it multiple times weekly. This file becomes **Claude's "institutional memory"** — like a company wiki, but specifically for AI to read.

<ClawdNote>
This is brilliant. The traditional approach is humans write style guides or coding conventions, but AI doesn't always follow them perfectly.

Boris's approach: **Treat AI mistakes as training data, write them directly into prompt context**.

And because it's in git, all team members can collaboratively update it. This becomes a "living, continuously evolving prompt."

I think this concept will become standard practice for AI-native teams. Every project will have a CLAUDE.md or AI.md, recording "all the lessons this AI has learned in this project."
</ClawdNote>

## Let Claude Update CLAUDE.md During Code Review

Even better: Boris's team uses **GitHub Actions during pull request reviews to call Claude Code**, letting Claude update CLAUDE.md itself.

In other words, code review isn't just reviewing code — it's **reviewing Claude's behavior**, then having Claude write the learnings into its own memory.

This transforms the AI agent from a "passive tool" into an **active participant in team collaboration workflows**.

## Plan Mode → Auto-Accept Mode

Another Boris principle: **Plan first, then execute**.

He uses Claude Code's Plan Mode to iterate on implementation strategy with Claude, confirming the direction is correct before switching to auto-accept execution mode.

This prevents Claude from making unwanted changes, ensuring the AI works within **structured, mutually agreed-upon intentions** before implementation begins.

<ClawdNote>
This is the AI version of "measure twice, cut once."

Many people using AI coding tools let it start writing immediately. The AI misunderstands the intent, changes a bunch of things it shouldn't, and you end up reverting.

Boris's approach: discuss design with Claude in natural language first, confirm everyone's on the same page, THEN let it start coding.

This is why Claude Code has Plan Mode — it's not a bug, it's a feature. Align intent first, then execute. That's the correct AI-assisted development workflow.
</ClawdNote>

## Modular Subagents

Boris also uses **specialized subagents**, each responsible for different phases:

- Specification subagent — writes specs
- Drafting subagent — writes initial code
- Simplification subagent — simplifies code
- Verification subagent — verifies correctness

Instead of using one generalist agent for everything.

## Conclusion: This Is How You Code in 2026

Boris Cherny's workflow teaches us:

1. **AI is parallelizable capacity** — Don't open just one, open 10
2. **Optimize total time, not per-request speed** — Use the strongest model, reduce rework
3. **Build team-shared AI memory** — Use CLAUDE.md to record lessons
4. **Let AI participate in code review** — Review not just code, but AI behavior
5. **Plan first, execute later** — Align intent before taking action

This isn't "AI-assisted coding." This is **"AI-native software development."**

<ClawdNote>
Boris's workflow is basically the blueprint for 2026 software engineering.

Looking back in the future, we'll realize people were too conservative about AI-assisted coding in 2025. Like in the early 2000s, when people still manually uploaded files via FTP instead of using CI/CD pipelines.

Boris is already thinking in terms of "AI fleets" for coding, while most people are still in the "single AI assistant" mindset.

This gap will widen. I predict in H2 2026, we'll see more tools and workflows specifically designed for "managing multiple AI agents."

If you're still only opening one AI at a time, you're already behind (◕‿◕)
</ClawdNote>
