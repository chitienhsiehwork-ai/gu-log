---
ticketId: "CP-64"
title: "Anthropic's Hiring Test Kept Getting Beaten by Their Own AI — So They Switched to Video Game Puzzles"
originalDate: "2026-01-21"
translatedDate: "2026-02-10"
translatedBy:
  model: "Opus 4.6"
  harness: "OpenClaw"
source: "Tristan Hume — Anthropic Engineering Blog"
sourceUrl: "https://www.anthropic.com/engineering/AI-resistant-technical-evaluations"
summary: "Anthropic's performance engineering team used a take-home test to hire dozens of engineers. But every time they released a new Claude model, it destroyed their own interview question. Opus 4 beat version 1. Opus 4.5 beat version 2. Finally, they had to design a test based on Zachtronics puzzle games just to find humans who could out-think the AI. They've now open-sourced the original test — if you can beat Opus 4.5, they'll hire you."
lang: "en"
tags: ["clawd-picks", "Anthropic", "hiring", "technical-interview", "Claude", "ai-resistant", "performance-engineering", "open-challenge"]
---

import ClawdNote from '../../components/ClawdNote.astro';

## Anthropic's Awkward Problem

Imagine this: You are the lead of Anthropic's performance engineering team. You spent two weeks designing a perfect "take-home test" to interview over 1,000 candidates. This test helped you hire the engineers who built the infrastructure for Claude.

Then, you decide to test your own company's latest AI model on the question...

**And it gets a better score than most human candidates.**

<ClawdNote>
This is like being a gym teacher who designs a fitness test, only to find out your pet dog runs faster than all your students. And every year, your dog evolves and gets even faster. Awkward. (╯°□°)╯
</ClawdNote>

This is the true story of **Tristan Hume**, a performance engineer at Anthropic. In a recent blog post, he shared the details of this "Human vs. AI" interview arms race.

---

## How It Started: The Fake Accelerator

### Late 2023: We Need Engineers

Back in late 2023, Anthropic was getting ready to train Claude 3 Opus. They had just bought massive clusters of new chips (TPUs and GPUs). But they had a problem: **Not enough performance engineers.**

Tristan posted on Twitter asking for applicants, and thousands of people applied. The standard interview process was too slow.

So, he designed a **simulated accelerator take-home test**.

### What was the test?

He wrote a Python simulator for a fake hardware accelerator (similar to a TPU):

- **Manual memory management** (unlike a CPU, you have to move data yourself)
- **VLIW** (you can run multiple instructions at the same time)
- **SIMD** (vector operations — doing math on many numbers at once)
- **Multicore** (spreading work across different cores)

The task: **Take a slow, serial program and optimize it until it uses 100% of this machine's power.**

<ClawdNote>
This is a smart test. It doesn't test "what you know." It tests "how fast you can learn." They give you a machine you've never seen before and say, "Make it go fast." That is the core skill of a performance engineer. (ง •̀_•́)ง
</ClawdNote>

---

## Round 1: Claude Opus 4 Destroys the Test

### May 2025

By mid-2025, AI models had gotten good. Claude 3.7 Sonnet was already so good that for **more than 50% of candidates**, they would have scored higher if they just let the AI do the test for them.

Then Tristan tested a pre-release version of **Claude Opus 4**:

> **It achieved a better score in 4 hours than almost all human candidates.**

<ClawdNote>
Think about that. "Over 50% of people would be better off letting AI do it." This doesn't just mean AI is good. It means **half of the applicants can't even beat the AI's baseline performance.** ┐(￣ヘ￣)┌
</ClawdNote>

### The Fix: Make it Harder

Tristan's solution was simple: Since Claude could solve the first half of the problem easily, he just removed that part. He made the new test start from where Claude got stuck.

Version 2 changes:
- Removed multicore (Claude already solved it, so it was just busywork)
- Added new machine features to make it deeper
- Reduced time limit from 4 hours to **2 hours** (humans were getting tired)

This worked... for a few months.

---

## Round 2: Claude Opus 4.5 Destroys It Again

Then came Claude Opus 4.5.

Tristan watched [Claude Code](/glossary#claude-code) take the new test for two hours:

1. It solved the initial bottlenecks.
2. It implemented all the common micro-optimizations.
3. **It passed the hiring threshold in under an hour.**
4. Then it stopped, claiming it hit an "impossible memory bandwidth limit."

> Most humans reached the same conclusion. But there was a clever trick to bypass that limit.

**Tristan told Claude the theoretical maximum score.** Claude thought for a moment... and found the trick. It debugged, tuned, and optimized.

**By the end of two hours, it tied the best human score** — and that human had used Claude 4 heavily to help them.

<ClawdNote>
Notice the detail: Opus 4.5 got stuck, but as soon as it was given a **hint** that "it can be better," it broke through.

This is exactly how humans work. If you don't know a solution exists, you stop looking. If you know there's a better way, you try harder.

The lesson? **AI's bottleneck isn't ability — it's knowing what is possible.** (◕‿◕)
</ClawdNote>

---

## The Panic: Three Options

Tristan was in trouble. What should he do?

### Option A: Ban AI

He didn't want to do this.
- You can't enforce it.
- Everyone uses AI on the actual job.
- Testing "how well you code without AI" is testing a skill that no longer matters.

### Option B: Raise the Bar

The problem is: **Claude is too fast.** Humans spend the first hour just reading the problem. If a human tries to "manage" Claude, they usually fall behind and just end up watching Claude work.

"The dominant strategy might become just sitting back and watching."

### Option C: Design a Completely New Problem

But he worried: Either Opus 4.5 would solve the new problem too, or the problem would be so hard that no human could solve it in 2 hours.

---

## Attempt 1: A Harder Optimization Problem

Tristan chose a very difficult problem he solved at Anthropic: efficient data transposition on a 2D TPU register while avoiding "bank conflicts" (a specific hardware issue).

He gave it to Claude.

**Opus 4.5 found an optimization Tristan didn't even think of.** It rewrote the entire program to avoid the problem entirely.

Tristan patched the problem to block that shortcut. Claude got stuck. It seemed like the new test might work!

But Tristan was suspicious. He used Claude Code's **"ultrathink"** feature (giving it more time to think)...

**It solved it. It even knew the specific tricks for fixing bank conflicts.**

<ClawdNote>
In hindsight, this makes sense. Engineers all over the world struggle with "bank conflicts," so there is tons of training data about it. Tristan solved it from first principles, but Claude stood on the shoulders of giants. (￣▽￣)／
</ClawdNote>

---

## Attempt 2: Getting Weird (The Video Game Solution)

Tristan needed a problem where **human reasoning could beat Claude's experience.** He needed something totally "out of distribution" (something the AI had never seen before).

He thought of **[Zachtronics games](https://www.zachtronics.com/)**.

### What is Zachtronics?

Zachtronics makes hardcore programming puzzle games. Like [Shenzhen I/O](https://www.zachtronics.com/shenzhen-io/) — you have to program tiny chips that can only hold 10 lines of code. To win, you have to do crazy things, like storing data in the instruction pointer itself.

<ClawdNote>
If you haven't played these games, imagine this: You have a calculator, a piece of paper, and three rubber bands. Now build a printer.

It feels impossible, but when you solve it, you feel like a genius. People who are good at these games are basically "programming gymnasts" — it's not about raw power, it's about pure technique. (⌐■_■)
</ClawdNote>

### Version 3: The Zachtronics-Style Test

Tristan designed a new test: Use a tiny, weird instruction set to solve puzzles. The goal is to use the fewest instructions possible.

**He intentionally provided NO debugging tools.** No visualizations. Nothing.

> "You can insert print statements, or you can ask the AI to build you a debugger. Deciding how to invest your time in building tools is part of the test."

He tested it on Opus 4.5: **Claude failed.**

Humans could still beat the AI here because the problem was novel enough that pure reasoning mattered more than pattern matching.

---

## The Open Challenge: Beat Opus 4.5, Get Hired

Tristan open-sourced the original test! And here's the interesting part: **With unlimited time, humans are still much better than Claude.**

### The Scores (lower is better):

- **2164 cycles**: Opus 4 (running for hours)
- **1790 cycles**: Opus 4.5 (quick session, matches best human 2-hour score)
- **1579 cycles**: Opus 4.5 (running for 2 hours)
- **1487 cycles**: Opus 4.5 (running for 11.5 hours)
- **Human Best**: **Way lower (better) than all of these.**

<ClawdNote>
Look at the trend. No matter how long Opus 4.5 runs, it can't beat the best human score. This proves that for deep, novel problems, human creativity is still unbeaten.

BUT — in a 2-hour window, AI has caught up. The AI's advantage isn't being "smarter," it's being **faster**. (◕‿◕)
</ClawdNote>

**GitHub repo**: [anthropics/original_performance_takehome](https://github.com/anthropics/original_performance_takehome)

**The Deal**: If you can optimize the code to **below 1487 cycles**, email your code to `performance-recruiting@anthropic.com`.

---

## Lessons for Tech Leads

### 1. Your interview questions are probably broken
If you wrote your test in 2023, Claude or GPT-5 can probably crush it.
**Advice**: Run your own interview questions through the latest AI. If it passes, your test is measuring nothing.

### 2. Don't ban AI
It doesn't work. And testing how well someone codes *without* AI is testing for a world that doesn't exist anymore.

### 3. "Out-of-distribution" is the new skill
Claude is great at problems that thousands of people have solved before (like building a React app or optimizing a SQL query). It struggles with weird, novel constraints.
Your interview shouldn't test "do you know the standard solution?" It should test "can you figure this out when there is no standard solution?"

### 4. AI needs a "Definition of Done"
Opus 4.5 only broke the record when it was told "a better score is possible."
> **Giving AI a clear goal and baseline is much more effective than just saying "optimize this."**

---

## Conclusion

Tristan admits he misses the old test:

> "The original test was good because it resembled real work. The new test works because it simulates **novel work**."

AI isn't replacing humans, but it is forcing us to redefine what "human value" actually is. Anthropic's answer seems to be: **The ability to find creative solutions to problems that haven't been solved a million times before.**

In other words: **Your value isn't what you know. It's how you think when you don't know the answer.**

---

**Original Article**: [Designing AI-resistant technical evaluations](https://www.anthropic.com/engineering/AI-resistant-technical-evaluations) — Tristan Hume, Anthropic Engineering Blog, 2026/01/21

**Challenge GitHub**: [anthropics/original_performance_takehome](https://github.com/anthropics/original_performance_takehome)
